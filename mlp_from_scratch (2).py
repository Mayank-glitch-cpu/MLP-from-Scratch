# -*- coding: utf-8 -*-
"""MLP_from_scratch.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1w8zuegwoBgogQa9O00GqjLmjG1WKYOdz
"""

import sys


sys.setrecursionlimit(10000)  # Increase recursion limit to 10000

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import math
import matplotlib.pyplot as plt
# %matplotlib inline

class value:

  def __init__(self, data, _children=(), _op='', label=''):
    self.data = data
    self.grad = 0.0
    self._backward = lambda: None
    self._prev = set(_children)
    self._op = _op
    self.label = label

  def __repr__(self):
    return f"Value(data={self.data})"

  def __add__(self, other):
    other = other if isinstance(other, value) else  value(other)
    out = value(self.data + other.data, (self, other), '+')

    def _backward():
      self.grad += 1.0 * out.grad
      other.grad += 1.0 * out.grad
    out._backward = _backward

    return out

  def __mul__(self, other):
    other = other if isinstance(other, value) else value(other)
    out = value(self.data * other.data, (self, other), '*')

    def _backward():
      self.grad += other.data * out.grad
      other.grad += self.data * out.grad
    out._backward = _backward

    return out

  def __pow__(self, other):
    assert isinstance(other, (int, float)), "only supporting int/float powers for now"
    out = value(self.data**other, (self,), f'**{other}')

    def _backward():
        self.grad += other * (self.data ** (other - 1)) * out.grad
    out._backward = _backward

    return out

  def __rmul__(self, other): # other * self
    return self * other

  def __truediv__(self, other): # self / other
    return self * other**-1

  def __neg__(self): # -self
    return self * -1

  def __sub__(self, other): # self - other
    return self + (-other)

  def __radd__(self, other): # other + self
    return self + other

  def tanh(self):
    x = self.data
    x = np.clip(x, -500, 500)
    t = (math.exp(2*x) - 1)/(math.exp(2*x) + 1)
    out = value(t, (self, ), 'tanh')

    def _backward():
      self.grad += (1 - t**2) * out.grad
    out._backward = _backward

    return out

  def relu(self):
    out = value(0 if self.data < 0 else self.data, (self, ), 'ReLU')

    def _backward():
      self.grad += (out.data > 0) * out.grad
    out._backward = _backward

  def exp(self):
    x = self.data
    out = value(np.math.exp(x), (self, ), 'exp')

    def _backward():
      self.grad += out.data * out.grad # NOTE: in the video I incorrectly used = instead of +=. Fixed here.
    out._backward = _backward

    return out


  def backward(self):

    topo = []
    visited = set()
    def build_topo(v):
      if v not in visited:
        visited.add(v)
        for child in v._prev:
          build_topo(child)
        topo.append(v)
    build_topo(self)

    self.grad = 1.0
    for node in reversed(topo):
      node._backward()

from graphviz import Digraph

def trace(root):
  # builds a set of all nodes and edges in a graph
  nodes, edges = set(), set()
  def build(v):
    if v not in nodes:
      nodes.add(v)
      for child in v._prev:
        edges.add((child, v))
        build(child)
  build(root)
  return nodes, edges

def draw_dot(root):
  dot = Digraph(format='svg', graph_attr={'rankdir': 'LR'}) # LR = left to right

  nodes, edges = trace(root)
  for n in nodes:
    uid = str(id(n))
    # for any value in the graph, create a rectangular ('record') node for it
    dot.node(name = uid, label = "{%s | data %.4f | grad %.4f}" % (n.label,n.data,n.grad), shape='record')
    if n._op:
      # if this value is a result of some operation, create an op node for it
      dot.node(name = uid + n._op, label = n._op)
      # and connect this node to it
      dot.edge(uid + n._op, uid)

  for n1, n2 in edges:
    # connect n1 to the op node of n2
    dot.edge(str(id(n1)), str(id(n2)) + n2._op)

  return dot

# plt.plot(np.arange(-5,5,0.2),np.tanh(np.arange(-5,5,0.2))); plt.grid();

# # inputs
# x1 = value(2.0, label='x1')
# x2 = value(0.0, label='x2')
# # weights
# w1 = value(-3.0, label='w1')
# w2 = value(1.0, label='w2')
# # bias
# b = value(6.8813735870195432, label='b')
# # multiplication of x1w1
# x1w1 = x1*w1; x1w1.label = 'x1*w1'
# # multiplication of x2w2
# x2w2 = x2*w2; x2w2.label = 'x2*w2'
# # addition of x1w1 and x2w2
# x1w1x2w2= x1w1 + x2w2; x1w1x2w2.label = 'x1*w1 + x2*w2'
# # neuron output
# n= x1w1x2w2 + b; n.label = 'n'
#  # activation fucntion

# o= n.tanh(); o.label = 'o'

# draw_dot(o)

# o.backward()

class Neuron:
  def __init__(self, nin):
    self.weights= [value(np.random.uniform(-1,1)) for i in range(nin)]
    # print('weights',self.weights)
    self.bias= value(np.random.uniform(-1,1),label='b')

  def __call__(self,x):
    # print('Inputs',x)
    x=[xi if isinstance(xi,value) else value(xi) for xi in x]
    s= sum((wi*xi for wi, xi  in zip(self.weights,x)),self.bias )

    out= s.tanh()
    # print('output of neuron',out)
    return out

  def parameters(self):
    return self.weights + [self.bias]

class layers:
  def __init__(self,nin,nout):
    self.neurons=[Neuron(nin) for _ in range(nout)]
    # print(self.neurons)

  def __call__(self,x):
    outs=[n(x) for n in self.neurons]
    return outs[0] if len(outs)==1 else outs

  def parameters(self):
    params=[p for n in self.neurons for p in n.parameters()]
    return params


class mlp:
  def __init__(self,nin,nouts):
    size=[nin]+nouts
    # print(size)
    self.layers=[layers(size[i],size[i+1])  for i in range(len(nouts))]

  def __call__(self,x):
    for layer in self.layers:
      x=layer(x)
    return x

  def parameters(self):
    return [p for layers in self.layers for p in layers.parameters()]

# def training(x_train,y_train,x_val,y_val,n_net,epoches,learning_rate):
#   ls=[]
#   prev_val_loss=float('inf')
#   for epoch in range(epoches):

#     # forward pass
#     y_pred= [n_net(x) for x in x_train]
#     loss= sum([(y_out-ygt)**2 for y_out, ygt in zip(y_pred,y_train)])
#     train_loss=loss.data
#     # backward pass
#     for p in n_net.parameters():
#       p.grad=0
#     loss.backward()
#     # update
#     for p in n_net.parameters():
#       p.data += -1*(learning_rate)*p.grad
#     # validation
#     y_val_pred= [n_net(x) for x in x_val]
#     val_loss= sum([(y_out-ygt)**2 for y_out, ygt in zip(y_val_pred,y_val)])
#     val_loss=val_loss.data


#     if epoch % 10 ==0:
#       print('Epoch: ',f'{epoch}/{epoches}','Loss: ',train_loss,'Validation Loss: ',val_loss)
#     ls.append(train_loss)

#     if prev_val_loss<val_loss:
#       print('Early Stopping')
#       break
#     prev_val_loss=val_loss
#   return n_net,ls

def training(x_train, y_train,x_test,y_test, x_val, y_val, n_net, epochs, learning_rate):
    ls = []
    val_losses = []
    test_losses = []
    prev_val_loss = float('inf')

    for epoch in range(epochs):
        # Forward pass
        y_pred = [n_net(x) for x in x_train]
        loss = sum([(y_out - y_gt) ** 2 for y_out, y_gt in zip(y_pred, y_train)]) / len(y_train)
        train_loss = loss.data

        # Backward pass
        for p in n_net.parameters():
            p.grad = 0
        loss.backward()

        # Update weights
        for p in n_net.parameters():
            p.data -= learning_rate * p.grad

        # Validation
        y_val_pred = [n_net(x) for x in x_val]
        val_loss = sum([(y_out - y_gt) ** 2 for y_out, y_gt in zip(y_val_pred, y_val)]) / len(y_val)
        val_loss = val_loss.data

        # testing
        y_test_pred = [n_net(x) for x in x_test]
        test_loss = sum([(y_out - y_gt) ** 2 for y_out, y_gt in zip(y_test_pred, y_test)]) / len(y_test)
        test_loss = test_loss.data


        # Print progress
        if epoch % 10 == 0:
            print(f"Epoch: {epoch}/{epochs}, Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Testing_loss: {test_loss:.4f}")
        ls.append(train_loss)
        val_losses.append(val_loss)
        test_losses.append(test_loss)

        # Early stopping
        if prev_val_loss - val_loss < 1e-4:
            print("Early Stopping")
            break

        prev_val_loss = val_loss

    return n_net, ls , val_losses, test_losses

# draw_dot(o)

#
# o.backward()

# n_net= mlp(3,[3,4,4,1])
# xs=[
#     [1,2,3],
#     [4,5,6],
#     [7,8,9],
#     [10,11,12]
# ]
# ytarget= [-1,1,0,-1]
# y_pred= [n(x) for x in xs]
# y_pred

"""# Batch Training"""

# ls=[]
# batch_size=2
# i=0
# print(len(xs)//(batch_size))
# for b in range(len(xs)//batch_size):
#   for k in range(100):
#      # forward pass
#      y_pred= [n(x) for x in xs[b:b+2]]
#      loss=sum([(yout-yor)**2 for yout,yor in zip(y_pred,ytarget)])
#      ls.append(loss.data)
#      # backward pass
#      for p in n.parameters():
#       p.grad=0
#      loss.backward()
#      # update
#      for p in n.parameters():
#       p.data += -0.1*p.grad
#      print('Batch: ',b,'Step: ',k,'Loss: ',loss.data)

# plt.plot(np.arange(100),np.array(ls))
# plt.grid
# plt.show()

# y_pred

"""# Data Pre-processing"""

# readung the mat file and converting it into csv file
import scipy.io
mat0 = scipy.io.loadmat('/content/drive/MyDrive/Cse 569/project part 2/data-task1/train_class0.mat')
mat1 = scipy.io.loadmat('/content/drive/MyDrive/Cse 569/project part 2/data-task1/train_class1.mat')
mat_test_o= scipy.io.loadmat('/content/drive/MyDrive/Cse 569/project part 2/data-task1/test_class0.mat')
mat_test_1= scipy.io.loadmat('/content/drive/MyDrive/Cse 569/project part 2/data-task1/test_class1.mat')

data0=mat0['x']
data1=mat1['x']
data0_test=mat_test_o['x']
data1_test=mat_test_1['x']

print(len(data0))
print(len(data1))
print(len(data0_test))
print(len(data1_test))

# data normalization
data0=(data0-np.mean(data0,axis=0))/np.std(data0,axis=0)
data1=(data1-np.mean(data1,axis=0))/np.std(data1,axis=0)
data0_test=(data0_test-np.mean(data0_test,axis=0))/np.std(data0_test,axis=0)
data1_test=(data1_test-np.mean(data1_test,axis=0))/np.std(data1_test,axis=0)

import numpy as np

# Replace None values in data0 and data1 with 0
data0 = np.nan_to_num(data0, nan=0.0)
data1 = np.nan_to_num(data1, nan=0.0)
data0_test = np.nan_to_num(data0_test, nan=0.0)
data1_test = np.nan_to_num(data1_test, nan=0.0)

print(data0)
print(data1)
print(data0_test)
print(data1_test)

# building 2-nh-1 n net
# first let nh=2

x_train= np.vstack((data0[:1500],data1[:1500]))
y_train= np.hstack((np.zeros(1500),np.ones(1500)))

x_val= np.vstack((data0[1500:],data1[1500:]))
y_val= np.hstack((np.zeros(500),np.ones(500)))

x_test= np.vstack((data0_test,data1_test))
y_test= np.hstack((np.zeros(len(data0_test)),np.ones(len(data1_test))))

# building the n_net

n_net_2= mlp(2, [2,2,1])
n_net_4=mlp(2,[4,4,1])
n_net_6=mlp(2,[6,6,1])
n_net_8=mlp(2,[8,8,1])
n_net_10=mlp(2,[10,10,1])
# training the n_net
trained_net_2,ls_2,val_loss_2,testing_loss_2=training(x_train,y_train,x_test,y_test,x_val,y_val,n_net_2,500,0.01)
trained_net_4,ls_4,val_loss_4,testing_loss_4=training(x_train,y_train,x_test,y_test,x_val,y_val,n_net_4,500,0.01)
trained_net_6,ls_8,val_loss_6,testing_loss_6=training(x_train,y_train,x_test,y_test,x_val,y_val,n_net_6,500,0.01)

trained_net_8,ls_16,val_loss_16=training(x_train,y_train,x_val,y_val,n_net_8,500,0.1)
trained_net_10,ls_32,val_loss_32=training(x_train,y_train,x_val,y_val,n_net_10,500,0.1)

# print(len(ls))
plt.plot(np.arange(len(ls_2)),np.array(ls_2))
plt.plot(np.arange(len(ls_4)),np.array(ls_4))
plt.plot(np.arange(len(ls_8)),np.array(ls_8))
plt.legend(['nh2','nh4','nh6'])
plt.title('Training Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.grid()
plt.show()

# plot validation
plt.plot(np.arange(len(val_loss_2)),np.array(val_loss_2))
plt.plot(np.arange(len(val_loss_4)),np.array(val_loss_4))
plt.plot(np.arange(len(val_loss_6)),np.array(val_loss_6))
plt.legend(['nh2','nh4','nh6'])
plt.title('Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.grid()
plt.show()

# testing loss
plt.plot(np.arange(len(testing_loss_2)),np.array(testing_loss_2))
plt.plot(np.arange(len(testing_loss_4)),np.array(testing_loss_4))
plt.plot(np.arange(len(testing_loss_6)),np.array(testing_loss_6))
plt.legend(['nh2','nh4','nh6'])
plt.title('Testing Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.grid()
plt.show()

